{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modified based on code from authors below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "# Copyright (C)                                                       #\n",
    "# 2016 Shangtong Zhang(zhangshangtong.cpp@gmail.com)                  #\n",
    "# 2016 Tian Jun(tianjun.cpp@gmail.com)                                #\n",
    "# 2016 Artem Oboturov(oboturov@gmail.com)                             #\n",
    "# 2016 Kenta Shimada(hyperkentakun@gmail.com)                         #\n",
    "# Permission given to modify the code as long as you keep this        #\n",
    "# declaration at the top                                              #\n",
    "#######################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "class Bandit:\n",
    "    # @kArm: # of arms\n",
    "    # @epsilon: probability of non-greedy selection in epsilon-greedy algorithm\n",
    "    # @initial: initial value estimate for each action\n",
    "    # @stepSize: constant step-size for updating estimations\n",
    "    # @sampleAverages: if True, use sample averages to update estimations instead of constant step size\n",
    "    # @UCB: if not None, use Upper-Confidence-Bound algorithm to select action\n",
    "    # @gradient: if True, use gradient-based bandit algorithm\n",
    "    # @gradientBaseline: if True, use average reward as baseline for gradient based bandit algorithm\n",
    "    def __init__(self, kArm = 10, epsilon = 0., initial = 0., stepSize = 0.1, sampleAverages = False, UCBParam = None, \n",
    "                 gradient = False, gradientBaseline = False, trueReward = 0.):\n",
    "        self.k = kArm\n",
    "        self.stepSize = stepSize\n",
    "        self.sampleAverages = sampleAverages\n",
    "        self.indices = np.arange(self.k)\n",
    "        self.time = 0\n",
    "        self.UCBParam = UCBParam\n",
    "        self.gradient = gradient\n",
    "        self.gradientBaseline = gradientBaseline\n",
    "        self.averageReward = 0\n",
    "        self.trueReward = trueReward\n",
    "\n",
    "        # real reward for each action\n",
    "        self.qTrue = []\n",
    "\n",
    "        # estimation for each action\n",
    "        self.qEst = np.zeros(self.k)\n",
    "\n",
    "        # # of chosen times for each action\n",
    "        self.actionCount = []\n",
    "        \n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        # initialize real rewards with N(0,1) distribution and estimations with desired initial value\n",
    "        for i in range(0, self.k):\n",
    "            self.qTrue.append(np.random.randn() + trueReward)\n",
    "            self.qEst[i] = initial\n",
    "            self.actionCount.append(0)\n",
    "\n",
    "        self.bestAction = np.argmax(self.qTrue)\n",
    "\n",
    "    # get an action for this bandit, explore or exploit?\n",
    "    def getAction(self):\n",
    "        # explore\n",
    "        if self.epsilon > 0:\n",
    "            if np.random.binomial(1, self.epsilon) == 1:\n",
    "                return np.random.choice(self.indices)\n",
    "\n",
    "        # exploit\n",
    "        if self.UCBParam is not None:\n",
    "            UCBEst = self.qEst + self.UCBParam * np.sqrt(np.log(self.time + 1) / (np.asarray(self.actionCount) + 1))\n",
    "            return np.argmax(UCBEst)\n",
    "        if self.gradient:\n",
    "            expEst = np.exp(self.qEst)\n",
    "            self.actionProb = expEst / np.sum(expEst)\n",
    "            return np.random.choice(self.indices, p=self.actionProb)\n",
    "        return np.argmax(self.qEst)\n",
    "\n",
    "    # take an action, update estimation for this action\n",
    "    def takeAction(self, action):\n",
    "        # generate the reward under N(real reward, 1)\n",
    "        reward = np.random.randn() + self.qTrue[action]\n",
    "        self.time += 1\n",
    "        self.averageReward = (self.time - 1.0) / self.time * self.averageReward + reward / self.time\n",
    "        self.actionCount[action] += 1\n",
    "\n",
    "        if self.sampleAverages:\n",
    "            # update estimation using sample averages\n",
    "            self.qEst[action] += 1.0 / self.actionCount[action] * (reward - self.qEst[action])\n",
    "        elif self.gradient:\n",
    "            oneHot = np.zeros(self.k)\n",
    "            oneHot[action] = 1\n",
    "            if self.gradientBaseline:\n",
    "                baseline = self.averageReward\n",
    "            else:\n",
    "                baseline = 0\n",
    "            self.qEst = self.qEst + self.stepSize * (reward - baseline) * (oneHot - self.actionProb)\n",
    "        else:\n",
    "            # update estimation with constant step size\n",
    "            self.qEst[action] += self.stepSize * (reward - self.qEst[action])\n",
    "        return reward"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
